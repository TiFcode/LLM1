get_bert_embeddings() received sentence as input: [This is a sample sentence for BERT.]
Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]Downloading:  14%|█▍        | 32.8k/232k [00:00<00:00, 309kB/s]Downloading:  43%|████▎     | 99.3k/232k [00:00<00:00, 460kB/s]Downloading: 100%|██████████| 232k/232k [00:00<00:00, 1.00MB/s]
Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 209kB/s]
Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]Downloading:   7%|▋         | 33.8k/466k [00:00<00:01, 297kB/s]Downloading:  21%|██▏       | 99.3k/466k [00:00<00:00, 449kB/s]Downloading:  53%|█████▎    | 247k/466k [00:00<00:00, 810kB/s] Downloading: 100%|██████████| 466k/466k [00:00<00:00, 1.30MB/s]
Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]Downloading: 100%|██████████| 570/570 [00:00<00:00, 4.49MB/s]
Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]Downloading:   2%|▏         | 8.04M/440M [00:00<00:05, 80.4MB/s]Downloading:   4%|▍         | 18.5M/440M [00:00<00:04, 94.6MB/s]Downloading:   6%|▋         | 28.0M/440M [00:00<00:04, 93.1MB/s]Downloading:   9%|▉         | 39.1M/440M [00:00<00:03, 100MB/s] Downloading:  11%|█         | 49.2M/440M [00:00<00:04, 86.5MB/s]Downloading:  13%|█▎        | 58.9M/440M [00:00<00:04, 89.8MB/s]Downloading:  15%|█▌        | 68.1M/440M [00:00<00:04, 89.3MB/s]Downloading:  18%|█▊        | 77.2M/440M [00:00<00:04, 89.8MB/s]Downloading:  20%|██        | 88.6M/440M [00:00<00:03, 97.1MB/s]Downloading:  22%|██▏       | 99.1M/440M [00:01<00:03, 99.3MB/s]Downloading:  25%|██▍       | 109M/440M [00:01<00:03, 96.3MB/s] Downloading:  27%|██▋       | 120M/440M [00:01<00:03, 98.8MB/s]Downloading:  30%|██▉       | 130M/440M [00:01<00:03, 102MB/s] Downloading:  32%|███▏      | 142M/440M [00:01<00:02, 107MB/s]Downloading:  35%|███▍      | 154M/440M [00:01<00:02, 109MB/s]Downloading:  37%|███▋      | 165M/440M [00:01<00:02, 110MB/s]Downloading:  40%|████      | 176M/440M [00:01<00:02, 111MB/s]Downloading:  43%|████▎     | 188M/440M [00:01<00:02, 113MB/s]Downloading:  45%|████▌     | 199M/440M [00:02<00:03, 74.8MB/s]Downloading:  48%|████▊     | 210M/440M [00:02<00:02, 81.1MB/s]Downloading:  50%|█████     | 221M/440M [00:02<00:02, 88.1MB/s]Downloading:  53%|█████▎    | 232M/440M [00:02<00:02, 93.8MB/s]Downloading:  55%|█████▌    | 243M/440M [00:02<00:02, 98.3MB/s]Downloading:  58%|█████▊    | 254M/440M [00:02<00:01, 102MB/s] Downloading:  60%|██████    | 265M/440M [00:02<00:01, 95.4MB/s]Downloading:  63%|██████▎   | 276M/440M [00:02<00:01, 99.8MB/s]Downloading:  65%|██████▍   | 286M/440M [00:02<00:01, 101MB/s] Downloading:  67%|██████▋   | 297M/440M [00:03<00:01, 103MB/s]Downloading:  70%|██████▉   | 308M/440M [00:03<00:01, 105MB/s]Downloading:  72%|███████▏  | 319M/440M [00:03<00:01, 107MB/s]Downloading:  75%|███████▌  | 331M/440M [00:03<00:00, 111MB/s]Downloading:  78%|███████▊  | 342M/440M [00:03<00:01, 84.3MB/s]Downloading:  80%|███████▉  | 352M/440M [00:03<00:01, 83.8MB/s]Downloading:  82%|████████▏ | 361M/440M [00:03<00:00, 86.9MB/s]Downloading:  85%|████████▍ | 373M/440M [00:03<00:00, 95.4MB/s]Downloading:  87%|████████▋ | 385M/440M [00:03<00:00, 100MB/s] Downloading:  90%|████████▉ | 395M/440M [00:04<00:00, 48.1MB/s]Downloading:  92%|█████████▏| 403M/440M [00:04<00:00, 53.3MB/s]Downloading:  93%|█████████▎| 411M/440M [00:04<00:00, 57.8MB/s]Downloading:  95%|█████████▌| 420M/440M [00:04<00:00, 63.0MB/s]Downloading:  98%|█████████▊| 431M/440M [00:04<00:00, 74.9MB/s]Downloading: 100%|██████████| 440M/440M [00:04<00:00, 88.8MB/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Last hidden state shape: torch.Size([1, 10, 768])
Pooler output shape: torch.Size([1, 768])



Sentence1 == This is a sample sentence for BERT.
Sentence2 == BERT is used in this sample sentence.
Sentence3 == I love to play football.
get_sentence_similarity() received these sentences as input:
[This is a sample sentence for BERT.]
[BERT is used in this sample sentence.]
get_bert_embeddings() received sentence as input: [This is a sample sentence for BERT.]
get_bert_embeddings() received sentence as input: [BERT is used in this sample sentence.]
Similarity between sentence 1 and 2: 0.985588014125824
get_sentence_similarity() received these sentences as input:
[This is a sample sentence for BERT.]
[I love to play football.]
get_bert_embeddings() received sentence as input: [This is a sample sentence for BERT.]
get_bert_embeddings() received sentence as input: [I love to play football.]
Similarity between sentence 1 and 3: 0.9726170301437378
