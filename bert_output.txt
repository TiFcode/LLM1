get_bert_embeddings() received sentence as input: [This is a sample sentence for BERT.]
Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]Downloading:  15%|█▍        | 33.8k/232k [00:00<00:00, 300kB/s]Downloading:  43%|████▎     | 99.3k/232k [00:00<00:00, 454kB/s]Downloading: 100%|██████████| 232k/232k [00:00<00:00, 981kB/s] 
Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 195kB/s]
Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]Downloading: 100%|██████████| 466k/466k [00:00<00:00, 7.46MB/s]
Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]Downloading: 100%|██████████| 570/570 [00:00<00:00, 4.81MB/s]
Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]Downloading:   1%|▏         | 5.55M/440M [00:00<00:08, 53.2MB/s]Downloading:   4%|▍         | 17.0M/440M [00:00<00:04, 88.5MB/s]Downloading:   6%|▌         | 26.9M/440M [00:00<00:04, 93.1MB/s]Downloading:   8%|▊         | 37.0M/440M [00:00<00:04, 96.5MB/s]Downloading:  11%|█         | 46.7M/440M [00:00<00:04, 92.5MB/s]Downloading:  13%|█▎        | 56.0M/440M [00:00<00:04, 89.4MB/s]Downloading:  15%|█▍        | 65.8M/440M [00:00<00:04, 91.9MB/s]Downloading:  17%|█▋        | 76.8M/440M [00:00<00:03, 97.5MB/s]Downloading:  20%|█▉        | 86.5M/440M [00:01<00:06, 54.8MB/s]Downloading:  21%|██▏       | 94.2M/440M [00:01<00:05, 59.0MB/s]Downloading:  24%|██▎       | 104M/440M [00:01<00:04, 68.0MB/s] Downloading:  26%|██▋       | 116M/440M [00:01<00:03, 81.2MB/s]Downloading:  29%|██▉       | 128M/440M [00:01<00:03, 90.6MB/s]Downloading:  32%|███▏      | 141M/440M [00:01<00:03, 99.0MB/s]Downloading:  35%|███▍      | 152M/440M [00:01<00:02, 104MB/s] Downloading:  37%|███▋      | 163M/440M [00:01<00:02, 106MB/s]Downloading:  40%|███▉      | 174M/440M [00:01<00:02, 108MB/s]Downloading:  42%|████▏     | 186M/440M [00:02<00:02, 111MB/s]Downloading:  45%|████▍     | 198M/440M [00:02<00:02, 110MB/s]Downloading:  47%|████▋     | 209M/440M [00:02<00:02, 107MB/s]Downloading:  50%|█████     | 221M/440M [00:02<00:01, 112MB/s]Downloading:  53%|█████▎    | 233M/440M [00:02<00:01, 115MB/s]Downloading:  56%|█████▌    | 245M/440M [00:02<00:01, 111MB/s]Downloading:  58%|█████▊    | 256M/440M [00:02<00:01, 101MB/s]Downloading:  61%|██████    | 268M/440M [00:02<00:01, 105MB/s]Downloading:  63%|██████▎   | 278M/440M [00:02<00:01, 94.3MB/s]Downloading:  66%|██████▌   | 290M/440M [00:03<00:01, 99.7MB/s]Downloading:  69%|██████▊   | 302M/440M [00:03<00:01, 106MB/s] Downloading:  71%|███████   | 313M/440M [00:03<00:01, 91.2MB/s]Downloading:  73%|███████▎  | 323M/440M [00:03<00:01, 93.8MB/s]Downloading:  76%|███████▌  | 335M/440M [00:03<00:01, 99.3MB/s]Downloading:  79%|███████▊  | 346M/440M [00:03<00:00, 103MB/s] Downloading:  81%|████████  | 356M/440M [00:03<00:01, 82.7MB/s]Downloading:  83%|████████▎ | 367M/440M [00:03<00:00, 88.0MB/s]Downloading:  86%|████████▌ | 378M/440M [00:04<00:00, 93.3MB/s]Downloading:  88%|████████▊ | 388M/440M [00:04<00:00, 89.7MB/s]Downloading:  91%|█████████ | 399M/440M [00:04<00:00, 95.0MB/s]Downloading:  93%|█████████▎| 409M/440M [00:04<00:00, 97.4MB/s]Downloading:  95%|█████████▌| 420M/440M [00:04<00:00, 101MB/s] Downloading:  98%|█████████▊| 432M/440M [00:04<00:00, 106MB/s]Downloading: 100%|██████████| 440M/440M [00:04<00:00, 95.1MB/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Last hidden state shape: torch.Size([1, 10, 768])
Pooler output shape: torch.Size([1, 768])



Sentence1 == Today the weather will be fine.
Sentence2 == Today the weather will be nice.
Sentence3 == I am hungry.



get_sentence_similarity() received these sentences as input:
[Today the weather will be fine.]
[Today the weather will be nice.]
get_bert_embeddings() received sentence as input: [Today the weather will be fine.]
get_bert_embeddings() received sentence as input: [Today the weather will be nice.]
Similarity between sentence 1 and 2: 0.9974629878997803



get_sentence_similarity() received these sentences as input:
[Today the weather will be fine.]
[I am hungry.]
get_bert_embeddings() received sentence as input: [Today the weather will be fine.]
get_bert_embeddings() received sentence as input: [I am hungry.]
Similarity between sentence 1 and 3: 0.9811462759971619



